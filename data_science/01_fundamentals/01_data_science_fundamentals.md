# Data Science Project Implementation

- Solving complex problems using:
    - Application of statistical methods;
    - Machine learning algorithms;
    - Analytical techniques.
- Different techniques, tools, and strategies can be used, but the overall process remains essentially the same.
- Not all stages are always present in a project; some steps may be skipped depending on the project's requirements.
- 7 core stages of a data science project:
    1. Define the problem;
    2. Collect and store data;
    3. Data cleaning and preprocessing;
    4. Exploratory Data Analysis (EDA);
    5. Predictive/statistical modeling (ML);
    6. Evaluation and testing;
    7. Delivering results.

# Problem Definition

- First, we must understand the business objective in question.
- Then, define the project's objectives.
- Identify research questions.
- Analyze available data (if data is not available, it must be gathered before starting the project).
- Define success metrics to determine when the project has achieved its goal.
- Plan the project accordingly.

# Data Collection and Storage

- Accurate data is the foundation for analysis and insights.
- Stakeholder involvement is crucial for defining data requirements.
- Various data collection methods:
    - Surveys
    - Interviews
    - Observations
    - Records
    - APIs
    - Public databases
- Ensuring data quality is fundamental.
- Data cleaning and validation techniques help maintain data integrity and accuracy.
    - Data Scientists can process the data after retrieval.
    - Data Engineers can pre-process data in data pipelines.
- Efficient and secure data storage is crucial:
    - Relational databases;
    - Data lakes;
    - Data warehouses.
- Protecting data is essential:
    - Implement security measures such as encryption, access control, and privacy policies.
- Data collection and storage are typically the responsibility of Data Engineers, but understanding the process is valuable if no dedicated engineer is available.

# Data Preparation and Cleaning

- Data almost always requires cleaning and processing.
- Preparation involves transforming raw data into a format suitable for analysis and modeling:
    - Cleaning:
        - Removing null values, duplicates, and inconsistencies.
        - Normalizing and standardizing data.
    - Transformation:
        - Converting data into appropriate formats for analysis.
        - Aggregation, encoding, and creating new variables.
    - Integration/Combination:
        - Merging data from different sources.
        - ETL (Extract, Transform, Load) is frequently used to consolidate data from various systems.
    - Encoding:
        - Converting text variables into numerical values.
        - Modifying data without altering its information content.
    - Reduction:
        - Reducing large and complex datasets while retaining the most relevant information.
        - Feature selection - PCA (Principal Component Analysis) helps reduce complexity and improves analysis efficiency.
- Data = raw material; Information = structured data; Knowledge = interpretation and application of information to generate value.

# Exploratory Data Analysis (EDA)

- Initial analysis of data to understand its characteristics and patterns.
- Uses statistical and descriptive techniques, visualization tools, and identifies relationships between variables while detecting outliers.
- Helps analysts and scientists formulate hypotheses, choose appropriate processing methods, and prevent errors.
- Data visualization aids in preliminary analysis and generating insights.
- Clear and informative visual representations facilitate understanding for non-technical stakeholders such as managers, directors, and executives.

# Predictive/Statistical Modeling
