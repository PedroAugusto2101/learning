# What Is Artificial Intelligence?

## Understanding AI  

- A field of computer science focused on creating systems capable of performing tasks that require **human intelligence**.  
- Includes **learning, reasoning, pattern recognition, and natural language understanding**.  
- In short, AI aims to replicate **human cognitive abilities** in machines.  

--- 
# The Secret Behind AI   

## What Is AI Really?  

- AI is a set of **techniques** used to build systems that replicate **human cognitive abilities**.  

## How Is This Achieved?  

- **Mathematics** → AI relies on **mathematical calculations** applied to vast amounts of data using high computational power.  
- **Human Intelligence** → Arises from **neural synapses** in the brain.  
- **Machine Intelligence** → Achieved through **electrical currents** combined with **numerical computations**.  
- In essence, AI is built on **mathematical operations**, from simple to advanced, using **programming languages** (Python is the most widely used).  

## Machine Learning and Pattern Recognition  

- AI algorithms learn from **existing patterns** in data rather than creating new ones.  
- **Machine learning models** are trained on data and can then be used to make predictions or classifications.  

## AI Applications  

- **Image classification and object detection**  
- **Text generation**  
- **Anomaly detection in financial transactions**  
- **Sales forecasting**  
- **Disease prediction**  

---  
# Artificial Intelligence (AI) vs Machine Learning (ML) vs Deep Learning (DL) vs Generative AI (GenAI)  

## **Artificial Intelligence (AI)**  

- Broad field in computer science focused on **creating intelligent systems** that simulate human cognitive abilities.  
- Includes **rule-based systems, automation, and machine learning**.  

## **Machine Learning (ML)**  

- **Subset of AI** that enables systems to **learn from data** without being explicitly programmed.  
- Uses **statistical models** and **algorithms** to detect patterns and make predictions.  
- Example: Fraud detection in financial transactions.  

## **Deep Learning (DL)**  

- **Subset of ML**, based on **neural networks** with multiple layers (**deep neural networks**).  
- Requires **large datasets** and **high computational power**.  
- Excels in **image recognition, speech processing, and NLP (Natural Language Processing)**.  
- Example: Facial recognition systems.  

## **Generative AI (GenAI)**  

- **Subset of DL**, specialized in **creating new content** (text, images, audio, code).  
- Uses **transformer-based architectures** like **GPT (ChatGPT), DALL·E, Stable Diffusion**.  
- Example: AI-generated art and text-based chatbots.  

![img](../img/Screenshot%20from%202025-03-22%2013-21-51.png)

--- 
# Main Categories of AI   

## **Narrow AI (Weak AI)**  

- Designed to perform a **specific task**.  
- **Lacks consciousness** or understanding beyond its programmed functions.  
- **All existing AI models today** fall under Narrow AI.  
- Examples: **ChatGPT, image recognition, recommendation systems**.  

## **General AI (Strong AI or AGI - Artificial General Intelligence)**  

- **Theoretical AI** that would have the ability to **understand, learn, and apply knowledge** similarly to human intelligence.  
- **Has not been developed yet**.  
- **Ongoing research and debate** topic.  

## **Multimodal Generative Models**  

- **Not AGI**.  
- **Combines multiple Narrow AI models** into a single solution.  
- Example: AI models that process and generate **text, images, and audio simultaneously**.  

---  
# When Will We Have AGI?  

- **AI requires a balance between hardware and software** to function properly.  
- **With current software and hardware**, achieving AGI is **not yet possible**.  
- A **new strategy** for AI model development would be necessary.  
- **Hardware improvements** are also needed—currently, AI relies on **GPUs**, which are **expensive**.  

---  
# Common AI Techniques  

## Machine Learning  
- **Subfield of AI** that uses algorithms to **learn patterns** from data.  
- Makes **predictions or decisions** without being explicitly programmed for each task.  
- **Widely used in:** price prediction, classification, marketing, finance.  

## Deep Learning  
- **Subfield of Machine Learning** that uses **artificial neural networks** with multiple layers.  
- Applied to **complex tasks** involving **large amounts of data**.  
- **Examples:** computer vision, natural language processing (NLP).  

## Reinforcement Learning  
- **Subfield of AI** based on **trial-and-error learning**.  
- The agent receives **rewards or penalties** based on its actions.  
- The goal is to **maximize total rewards** over time.  
- **Widely applied in:** robotics, gaming, control systems, trading bots.

---  
# History and Evolution of AI  

## 1940-1950: First Mathematical Model  
- **1943:** First **mathematical model** of a neural network.  
- **1950:** **Turing Test** proposed to determine if a machine can exhibit intelligent behavior.  

## 1950-1960: Early Steps and the Birth of AI  
- **1956:** The term **"Artificial Intelligence"** was coined.  
- **1957:** Introduction of the **Perceptron**, a supervised learning algorithm for neural networks.  
- **1958:** **LISP** programming language developed, becoming fundamental for AI research.  

## 1960-1970: Optimism and Early Systems  
- **1965:** **ELIZA**, one of the first **natural language processing (NLP)** programs.  
- **1969:** The book **"Perceptrons"** highlights the limitations of early neural networks, leading to decreased interest in the approach.  

## 1970-1980: "AI Winter" and Expert Systems  
- **1970:** First **AI funding crisis** due to **unfulfilled promises and technological limitations**.  
- **1972:** Introduction of **Prolog**, a logic programming language.  
- **1980:** Development of **expert systems**, such as:  
  - **DENDRAL** (chemical analysis).  
  - **MYCIN** (medical diagnosis).  

## 1980-1990: AI Revitalization  
- **1986:** Development of the **backpropagation algorithm** for training neural networks.  
  - Still used today in **Deep Learning** models, the leading AI strategy.  
- **1987:** Second **AI funding crisis** due to the **failure of expert systems** to meet commercial expectations.  
  - One key issue was the **lack of sufficient data** to support these systems.  

## 1990-2000: AI in Everyday Life  
- **1997:** **IBM's Deep Blue** defeats the world chess champion.  
- **1999:** **Nvidia develops powerful GPUs (Graphics Processing Units)**, initially for rendering video games.  
  - GPUs later became the **driving force of AI evolution** due to their ability to **parallelize matrix operations**, essential for **artificial neural networks**.  

## 2000-2010: AI on the Internet, Big Data, and GPU Revolution  
- **2005:** The term **Big Data** is defined for the first time.  
  - Represents **large volumes of data** generated at **high speed and variety** → **fuel for AI**.  
- **2006:** The term **"Deep Learning"** is introduced, demonstrating its potential in **pattern recognition**.  
- **2009:** **GPUs gain prominence** in accelerating AI model training.  
  - **If Big Data was the fuel, GPUs were the ignition.**  

## 2010-2020: Advanced AI and Practical Applications  
- **2012:** The **AlexNet deep learning architecture (CNN - Convolutional Neural Network)** wins the **ImageNet** image recognition competition.  
  - Demonstrates the power of **deep neural networks**.  
- **2016:** **AlphaGo defeats the world champion of Go**, a game known for its **strategic complexity**.  
- **2017:** The paper **"Attention is All You Need"** introduces a **new way to learn sequences** based on context.  
  - Proposes the **Transformer architecture** and **Attention Mechanisms**, revolutionizing **natural language processing (NLP)**.  

## 2020-2025: AI Evolution and Real-World Applications  
- **2020:** Several models based on the **Transformer architecture** emerge, leading to a **new generation of AI systems**, including **GPT models**.  
- **2021:** AI becomes **widely used across industries**, including **healthcare and finance**.  
  - **Ethical concerns rise** regarding **privacy, bias, and job impact**.  
- **2022:** **OpenAI launches ChatGPT**, showcasing the **power of Generative AI** (which existed before but gained global attention).  
- **2023:** **Race to develop more powerful Large Language Models (LLMs)** intensifies.  
- **2024:** **MetaAI releases the most powerful open-source LLM**, featuring **405 billion parameters**.  

---
# IA and Data Science

![img](../img/Screenshot%20from%202025-03-25%2007-25-23.png)


